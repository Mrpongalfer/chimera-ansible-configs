### Nexus Reality Scan Initializing ###
Timestamp: Fri Apr 25 05:30:54 PM MDT 2025
=========================================

### System Information ###
User: pong
Hostname: pop-os
Current Directory: /home/pong/Projects/chimera-ansible-configs
----- OS Info -----
Distributor ID:	Pop
Description:	Pop!_OS 22.04 LTS
Release:	22.04
Codename:	jammy
----- Kernel Info -----
Linux pop-os 6.12.10-76061203-generic #202412060638~1743109366~22.04~1fce33b SMP PREEMPT_DYNAMIC Thu M x86_64 x86_64 x86_64 GNU/Linux
=========================================

### Ansible Environment ###
----- Ansible Version -----
ansible [core 2.17.10]
  config file = /home/pong/Projects/chimera-ansible-configs/ansible.cfg
  configured module search path = ['/home/pong/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /home/pong/ansible_venv/lib/python3.10/site-packages/ansible
  ansible collection location = /home/pong/.ansible/collections:/usr/share/ansible/collections
  executable location = /home/pong/ansible_venv/bin/ansible
  python version = 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0] (/home/pong/ansible_venv/bin/python3)
  jinja version = 3.1.6
  libyaml = True
----- Installed Collections -----

# /home/pong/.ansible/collections/ansible_collections
Collection                               Version
---------------------------------------- -------
community.docker                         4.5.2  
community.general                        10.5.0 
community.library_inventory_filtering_v1 1.1.0  
----- Python Packages (pip list) -----
Package         Version
--------------- -------
ansible-core    2.17.10
cffi            1.17.1
cryptography    44.0.2
git-filter-repo 2.47.0
Jinja2          3.1.6
MarkupSafe      3.0.2
packaging       24.2
pathspec        0.12.1
pip             25.0.1
pycparser       2.22
PyYAML          6.0.2
resolvelib      1.0.1
setuptools      59.6.0
yamllint        1.37.0
=========================================

### Git Repository Status ###
----- Git Status -----
On branch main
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   roles/quantum_orchestrator/tasks/main.yml

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	nexus_scan_20250425_173054.txt

no changes added to commit (use "git add" and/or "git commit -a")
----- Git Remotes -----
origin	https://github.com/Mrpongalfer/chimera-ansible-configs.git (fetch)
origin	https://github.com/Mrpongalfer/chimera-ansible-configs.git (push)
----- Git Branches -----
* main 6debbf1 fix(common): Add rsync package dependency for synchronize module
----- Git Log (Last 5) -----
* 6debbf1 (HEAD -> main, origin/main) fix(common): Add rsync package dependency for synchronize module
* a45ad31 feat: Add Quantum Orchestrator deployment role and update site
* 58a51d8 Fix(wizardpro_bootstrap): Correct src paths for Python/CSS skeleton files
* 66d384a Feat: Add wizardpro_bootstrap role and integrate into site.yml
* 00040ab Feat: Add wizardpro_bootstrap role with source files
=========================================

### Project Directory Structure ###
.
├── ansible.cfg
├── ansible.cfg.bak_vaultfix_2025-04-14_17:10:10
├── ansible_run.log
├── collections
│   └── requirements.yml
├── gather_nexus_info.sh
├── handlers
│   └── main.yml
├── host_vars
├── inventory
│   ├── group_vars
│   │   ├── all
│   │   ├── all.yml
│   │   ├── clients.yml
│   │   ├── server.yml
│   │   ├── server.yml.bak_promfinal_2025-04-14_18:55:37
│   │   ├── server.yml.bak_promfinal_2025-04-14_18:56:02
│   │   ├── server.yml.bak_promfinal2_2025-04-14_18:57:18
│   │   ├── server.yml.bak_promvars_2025-04-14_18:50:44
│   │   └── server.yml.bak_promvars2_2025-04-14_18:52:36
│   ├── hosts_generated
│   ├── hosts_generated.bak_2025-04-14_12:57:06
│   ├── hosts_generated.bak_2025-04-14_12:58:14
│   ├── hosts_generated.bak_groupfix_2025-04-14_17:42:40
│   ├── hosts_generated.bak_groupfix2_2025-04-14_18:44:55
│   └── hosts_generated.bak_keyfix_2025-04-14_17:27:29
├── Justfile
├── nexus_scan_20250424_200122.txt
├── nexus_scan_20250425_173054.txt
├── playbooks
│   ├── base_python_setup.yml
│   ├── debug_vars.yml
│   ├── docker_setup.yml
│   └── setup_client_for_mgmt.yml
├── README.md
├── requirements-server-venv.txt
├── roles
│   ├── common
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── tests
│   │   └── vars
│   ├── docker
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── tests
│   │   └── vars
│   ├── grafana
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── templates
│   │   ├── tests
│   │   └── vars
│   ├── loki
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── templates
│   │   ├── tests
│   │   └── vars
│   ├── node_exporter
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── tests
│   │   └── vars
│   ├── prometheus
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── templates
│   │   ├── tests
│   │   └── vars
│   ├── prompt_debug
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── tests
│   │   └── vars
│   ├── promtail
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── templates
│   │   ├── tests
│   │   └── vars
│   ├── quantum_orchestrator
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── tasks
│   │   ├── templates
│   │   └── vars
│   ├── security
│   │   ├── defaults
│   │   ├── handlers
│   │   ├── meta
│   │   ├── README.md
│   │   ├── tasks
│   │   ├── templates
│   │   ├── tests
│   │   └── vars
│   └── wizardpro_bootstrap
│       ├── defaults
│       ├── files
│       ├── handlers
│       ├── meta
│       ├── README.md
│       ├── tasks
│       ├── templates
│       ├── tests
│       └── vars
├── site.yml
├── site.yml.bak_playvars_2025-04-14_19:00:59
├── site.yml.bak_varsinplay_2025-04-14_19:14:49
├── tasks
│   └── main.yml
├── test_infra.yml
├── tests
│   └── goss
│       └── goss.yml
└── vars
    └── common_packages.yml

95 directories, 46 files
=========================================

### Key Configuration File Contents ###
----- Content of: ansible.cfg -----
# Ansible Configuration File - Managed by Chimera Setup

[defaults]
inventory = ./inventory/hosts_generated
host_key_checking = False
vault_password_file = /home/aiseed/.ansible_vault_pass
# Note: Vault password file path set above

# Add other sections like [privilege_escalation] or [ssh_connection] if needed
# [privilege_escalation]
# become=True
# become_method=sudo
# become_user=root
# become_ask_pass=False

--- End of: ansible.cfg ---

----- Content of: hosts_generated -----
# Inventory generated by bootstrap, corrected by Chimera v2.7 (v1.1)
[server]
192.168.0.95 ansible_user=aiseed

[clients]
192.168.0.96 ansible_user=pong

[control_node]
# Use 'localhost' if you want to manage Pop!_OS itself with Ansible using this inventory
# localhost ansible_connection=local ansible_user=pong
--- End of: hosts_generated ---

----- Content of: site.yml -----
# File: /home/pong/Projects/chimera-ansible-configs/site.yml
# Version: 2.0 FINAL - Deploys Monitoring Stack & Quantum Orchestrator

---
- name: Pre-flight APT Cleanup and Validation on Server(s)
  hosts: server
  become: yes
  gather_facts: no
  tags: [always] # Ensure pre-flight runs even when tagging plays
  tasks:
    # --- LazyGit PPA Cleanup ---
    - name: Attempt removal of problematic lazygit PPA via module
      ansible.builtin.apt_repository:
        repo: "ppa:lazygit-team/release"
        state: absent
      ignore_errors: yes

    - name: Force removal of lazygit PPA sources list file (.list format)
      ansible.builtin.file:
        path: "/etc/apt/sources.list.d/lazygit-team-ubuntu-release-{{ ansible_distribution_release | default('noble') }}.list"
        state: absent
      ignore_errors: yes

    - name: Force removal of lazygit PPA sources file (.sources format)
      ansible.builtin.file:
        path: "/etc/apt/sources.list.d/lazygit-team-ubuntu-release-{{ ansible_distribution_release | default('noble') }}.sources"
        state: absent
      ignore_errors: yes

    # --- Docker APT Config Cleanup ---
    - name: Remove potential conflicting Docker sources file (.list)
      ansible.builtin.file:
        path: "/etc/apt/sources.list.d/docker.list"
        state: absent
      ignore_errors: yes

    - name: Remove potential conflicting Docker sources file (.sources)
      ansible.builtin.file:
        path: "/etc/apt/sources.list.d/docker.sources"
        state: absent
      ignore_errors: yes

    - name: Remove potential conflicting Docker keyring file (.gpg)
      ansible.builtin.file:
        path: "/etc/apt/keyrings/docker.gpg"
        state: absent
      ignore_errors: yes

    - name: Remove potential conflicting Docker keyring file (.asc)
      ansible.builtin.file:
        path: "/etc/apt/keyrings/docker.asc"
        state: absent
      ignore_errors: yes
    # --- End Docker Cleanup ---

    - name: Clean APT package cache thoroughly
      ansible.builtin.command: apt-get clean -y
      changed_when: false

    - name: Update APT cache explicitly AFTER cleanup to validate
      ansible.builtin.apt:
        update_cache: yes
      register: preflight_apt_update_result
      retries: 1
      delay: 3
      until: preflight_apt_update_result is succeeded

# --- Main Configuration Play ---
- name: Apply common configuration to all hosts
  hosts: all
  become: yes
  gather_facts: yes
  vars:
    # --- Common Variables ---
    system_timezone: "America/Denver"
    admin_users:
      - { username: aiseed, shell: /bin/bash }
      - { username: pong, shell: /bin/bash }
    common_pkgs:
      - build-essential
      - python3-dev
      - curl
      - wget
      - git
      - vim
      - tmux
      - htop
      - net-tools
      - dnsutils
      - unzip
      - ca-certificates
      - gnupg
      - python3-pip
      - python3-venv
      - chrony
      - rsync
      - libpq-dev # Added for psycopg2 system dependency

    # --- Security Variables ---
    security_ssh_port: 22
    security_fail2ban_enabled: false
    fail2ban_ignoreip: "127.0.0.1/8 ::1 192.168.0.0/24"
    monitoring_allowed_sources: "192.168.0.0/24"

    # --- Port Definitions ---
    prometheus_port: 9091
    loki_port: 3100
    grafana_port: 3000
    node_exporter_port: 9100
    qo_app_port: 8000 # Quantum Orchestrator API Port

    # --- UFW Rules ---
    # Rules applied by 'security' role based on group membership
    server_ufw_allow_rules:
      - {
          comment: "Allow SSH from Client",
          port: "{{ security_ssh_port }}",
          proto: tcp,
          rule: allow,
          src: "192.168.0.96",
        }
      - {
          comment: "Allow SSH from Localhost",
          port: "{{ security_ssh_port }}",
          proto: tcp,
          rule: allow,
          src: "127.0.0.1",
        }
      - {
          comment: "Allow Prometheus from Monitoring Subnet",
          port: "{{ prometheus_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        }
      - {
          comment: "Allow Loki from Monitoring Subnet",
          port: "{{ loki_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        }
      - {
          comment: "Allow Grafana from Monitoring Subnet",
          port: "{{ grafana_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        }
      - {
          comment: "Allow Node Exporter from Monitoring Subnet",
          port: "{{ node_exporter_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        }
      - {
          comment: "Allow Quantum Orchestrator API",
          port: "{{ qo_app_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        } # Corrected syntax
    client_ufw_allow_rules:
      - {
          comment: "Allow SSH from Server",
          port: "{{ security_ssh_port }}",
          proto: tcp,
          rule: allow,
          src: "192.168.0.95",
        }
      - {
          comment: "Allow SSH from Localhost",
          port: "{{ security_ssh_port }}",
          proto: tcp,
          rule: allow,
          src: "127.0.0.1",
        }
      - {
          comment: "Allow Node Exporter from Monitoring Subnet",
          port: "{{ node_exporter_port }}",
          proto: tcp,
          rule: allow,
          src: "{{ monitoring_allowed_sources }}",
        }

    # --- Docker Variables ---
    docker_users_to_group:
      - aiseed
      - pong

    # --- Promtail Variables ---
    loki_server_url: "http://192.168.0.95:{{ loki_port }}/loki/api/v1/push"

  roles:
    - role: common
      tags: [common]
    - role: security
      tags: [security]
    - role: docker
      tags: [docker]
    - role: node_exporter
      tags: [monitoring, node_exporter]
    - role: promtail
      tags: [monitoring, promtail]

# --- Server-Specific Configuration ---
- name: Configure server-specific services (Monitoring Stack & Quantum Orchestrator)
  hosts: server
  become: yes
  gather_facts: no
  vars:
    # Define variables needed specifically by roles running only on the server

    # Explicitly define ports here again to ensure scope for roles
    # (This proved necessary during debugging)
    prometheus_port: 9091
    loki_port: 3100
    grafana_port: 3000
    node_exporter_port: 9100 # Needed by Prometheus template
    qo_app_port: 8000 # Needed by Quantum Orchestrator role / UFW

    # User/Group for container files/processes on server
    docker_run_user: "aiseed"
    docker_run_group: "aiseed"

    # Prometheus Paths & Config
    prometheus_base_dir: "/opt/docker/prometheus"
    prometheus_config_dir: "{{ prometheus_base_dir }}/config"
    prometheus_data_dir: "{{ prometheus_base_dir }}/data"
    prometheus_compose_dir: "{{ prometheus_base_dir }}"
    prometheus_template_name: "docker-compose.yml.j2"
    prometheus_compose_file_path: "{{ prometheus_compose_dir }}/docker-compose.yml"
    prometheus_config_template: "prometheus.yml.j2"
    prometheus_config_file_path: "{{ prometheus_config_dir }}/prometheus.yml"

    # Loki Paths & Config
    loki_base_dir: "/opt/docker/loki"
    loki_config_dir: "{{ loki_base_dir }}/config"
    loki_data_dir: "{{ loki_base_dir }}/data"
    loki_compose_dir: "{{ loki_base_dir }}"
    loki_compose_file_path: "{{ loki_compose_dir }}/docker-compose.yml"
    loki_config_template: "loki-config.yml.j2"
    loki_config_file_path: "{{ loki_config_dir }}/loki-config.yml"

    # Grafana Paths & Config
    grafana_base_dir: "/opt/docker/grafana"
    grafana_config_dir: "{{ grafana_base_dir }}/config"
    grafana_data_dir: "{{ grafana_base_dir }}/data"
    grafana_compose_dir: "{{ grafana_base_dir }}"
    grafana_compose_file_path: "{{ grafana_compose_dir }}/docker-compose.yml"
    grafana_admin_password: "{{ vault_grafana_admin_password | default('ChangeMePlease!') }}"

    # Quantum Orchestrator Vars
    qo_local_code_path: "/home/pong/Projects/replitwork" # Source code on control node
    qo_base_dir: "/opt/docker/quantum_orchestrator"
    qo_deploy_dir: "{{ qo_base_dir }}"
    qo_container_port: 8000 # Internal port defined in Dockerfile/App
    qo_llm_model: "mixtral" # Default local model for Ollama service

  roles:
    - role: prometheus
      tags: [monitoring, prometheus]
    - role: loki
      tags: [monitoring, loki]
    - role: grafana
      tags: [monitoring, grafana]
    - role: quantum_orchestrator # Correct role name
      tags: [app, quantum_orchestrator]

# --- Client-Specific Configuration ---
- name: Configure client-specific settings
  hosts: clients
  become: yes
  gather_facts: no
  vars:
    placeholder_client_var: true # Example if client needs specific vars
    # No QO vars needed here
  roles:
    # Add any client-specific roles here if needed later
    # - role: some_client_role
    - role: prompt_debug # Keeping placeholder for now
      tags: [debug] # Assign tag
--- End of: site.yml ---

----- Content of: common_packages.yml -----
---
# Explicitly defined variables for common role
common_pkgs:
  - build-essential
  - python3-dev
  - curl
  - wget
  - git
  - vim
  - tmux
  - htop
  - net-tools
  - dnsutils
  - unzip
  - ca-certificates
  - gnupg
--- End of: common_packages.yml ---

----- Content of: all.yml -----
---
# Simplified for debugging variable loading - v1.1
common_pkgs:
  - build-essential
  - python3-dev
  - curl
  - wget
  - git
  - vim
  - tmux
  - htop
  - net-tools
  - dnsutils
  - unzip
  - ca-certificates
  - gnupg
--- End of: all.yml ---

----- Content of: server.yml -----
# Corrected server.yml - Version 1.8 (Final Prometheus Vars)
# Define ALL required vars here, including port override
prometheus_port_override: 9091 # Changed default port

# --- Prometheus Role Variables (Defined Here for Handlers) ---
prometheus_config_dir: "/opt/docker/prometheus/config"
prometheus_data_dir: "/opt/docker/prometheus/data"
prometheus_compose_dir: "/opt/docker/prometheus"
docker_run_user: "aiseed" # User owning files/running compose on server - ADJUST IF NEEDED
docker_run_group: "aiseed" # Group owning files on server - ADJUST IF NEEDED
prometheus_template_name: "docker-compose.yml.j2"
prometheus_compose_file_path: "{{ prometheus_compose_dir }}/docker-compose.yml"
prometheus_config_template: "prometheus.yml.j2"
prometheus_config_file_path: "{{ prometheus_config_dir }}/prometheus.yml"
node_exporter_port: 9100 # For scrape config example

# --- Other Server Vars (Ensure consistency) ---
server_ufw_allow_rules:
  - comment: Allow SSH from Client 'pong@pop-os'
    port: '{{ security_ssh_port | default(22) }}'
    proto: tcp
    rule: allow
    src: 192.168.0.96
  - comment: Allow SSH from Localhost
    port: '{{ security_ssh_port | default(22) }}'
    proto: tcp
    rule: allow
    src: 127.0.0.1
  - comment: Allow Prometheus from Client Subnet # Uses port override
    port: '{{ prometheus_port_override | default(9091) }}'
    proto: tcp
    rule: allow
    src: '{{ monitoring_allowed_sources | default("192.168.0.0/24") }}'

# Add any OTHER essential variables originally in server.yml below:

--- End of: server.yml ---

----- Content of: clients.yml -----
# /opt/architect_configs/group_vars/clients.yml
---
# Client Specific UFW Rules - MINIMAL SECURE DEFAULTS

client_ufw_allow_rules:
  # Allow SSH ONLY from the server IP and localhost for management
  - { rule: allow, port: "{{ security_ssh_port | default(22) }}", proto: tcp, src: '192.168.0.95', comment: "Allow SSH from Server 'aiseed@thosedataguys-s'" }
  - { rule: allow, port: "{{ security_ssh_port | default(22) }}", proto: tcp, src: '127.0.0.1', comment: "Allow SSH from Localhost" }

  # No other incoming ports allowed by default for client workstation.
--- End of: clients.yml ---

----- List Vault Files -----
./inventory/group_vars/all/vault.yml
--- End Vault List ---

----- Content of: main.yml -----
# /opt/architect_configs/roles/common/tasks/main.yml
# Version 1.1 - Added debug task
---
# Tasks applied to all hosts for common baseline configuration

- name: Update apt cache if older than 1 hour
  ansible.builtin.apt:
    update_cache: yes
    cache_valid_time: 3600 # Only update if cache is > 1 hour old
  changed_when: false # Prevent this task alone from reporting 'changed' status
  tags: [common, packages, apt]


- name: Install common packages defined in group_vars/all.yml
  ansible.builtin.apt:
    name: "{{ common_pkgs }}" # Uses the variable list from group_vars/all.yml
    state: present
  tags: [common, packages, apt]

- name: Set system timezone
  community.general.timezone:
    name: "{{ system_timezone }}" # Uses the variable
  tags: [common, system, timezone]

# --- NTP Configuration (using Chrony) ---
- name: Ensure chrony NTP client is installed
  ansible.builtin.apt:
    name: chrony
    state: present
  tags: [common, system, ntp]

- name: Ensure chrony service is enabled and running
  ansible.builtin.service:
    name: chrony # Service name might be chronyd on some systems, check service status if fails
    state: started
    enabled: yes
  tags: [common, system, ntp]

# Basic user check - ensures specified users exist with correct shell
- name: Ensure admin users exist
  ansible.builtin.user:
    name: "{{ item.username }}"
    shell: "{{ item.shell }}"
    state: present
  loop: "{{ admin_users }}" # Loop through the list defined in group_vars/all.yml
  tags: [common, users]

# Add other common tasks below as needed:

--- End of: main.yml ---

----- Content of: main.yml -----
---
# Tasks for security role - v2.0 Idempotent UFW Reset
- name: Deploy hardened sshd_config from template
  ansible.builtin.template:
    src: sshd_config.j2
    dest: /etc/ssh/sshd_config
    owner: root
    group: root
    mode: '0600'
  notify: Restart sshd
  tags: [security, ssh]

- name: Ensure ufw package is installed
  ansible.builtin.apt:
    name: ufw
    state: present
  tags: [security, ufw]

# --- BEGIN IDEMPOTENT UFW RESET APPROACH ---
# This block ensures UFW is reset ONLY if it's currently active and managed
# Avoids errors about backup files and unnecessary resets

- name: Check current UFW status
  ansible.builtin.command: ufw status
  register: ufw_status_check
  changed_when: false
  failed_when: false # Continue even if inactive
  become: true
  tags: [security, ufw]

# Only attempt reset if UFW is currently active (prevents issues on first run)
# Further checks could be added here if more complex state detection needed
- name: Reset UFW to defaults if it is currently active
  ansible.builtin.command: ufw --force reset
  when: "'Status: active' in ufw_status_check.stdout"
  changed_when: true # Reset always changes state conceptually
  become: true
  notify: Reload ufw
  tags: [security, ufw]
# --- END IDEMPOTENT UFW RESET APPROACH ---

- name: Set UFW logging state
  community.general.ufw:
    logging: 'on'
  notify: Reload ufw
  tags: [security, ufw]

- name: Set UFW default policies (Deny Incoming)
  community.general.ufw:
    direction: "{{ item.direction }}"
    policy: "{{ item.policy }}"
  loop:
    - { direction: incoming, policy: deny }
    - { direction: outgoing, policy: allow }
    - { direction: routed, policy: deny }
  notify: Reload ufw
  tags: [security, ufw]

- name: Apply Server-specific UFW allow rules
  community.general.ufw:
    rule: "{{ item.rule | default('allow') }}"
    direction: "{{ item.direction | default('in') }}"
    from_ip: "{{ item.src | default('any') }}"
    to_ip: "{{ item.dest | default('any') }}"
    port: "{{ item.port | default(omit) }}"
    proto: "{{ item.proto | default('any') }}"
    comment: "{{ item.comment | default(omit) }}"
  loop: "{{ server_ufw_allow_rules | default([]) }}"
  when: inventory_hostname in groups['server']
  notify: Reload ufw
  tags: [security, ufw]

- name: Apply Client-specific UFW allow rules
  community.general.ufw:
    rule: "{{ item.rule | default('allow') }}"
    direction: "{{ item.direction | default('in') }}"
    from_ip: "{{ item.src | default('any') }}"
    to_ip: "{{ item.dest | default('any') }}"
    port: "{{ item.port | default(omit) }}"
    proto: "{{ item.proto | default('any') }}"
    comment: "{{ item.comment | default(omit) }}"
  loop: "{{ client_ufw_allow_rules | default([]) }}"
  when: inventory_hostname in groups['clients']
  notify: Reload ufw
  tags: [security, ufw]

- name: Ensure ufw is enabled and running
  community.general.ufw:
    state: enabled
  tags: [security, ufw]

# --- Fail2ban Section ---
# (Assuming Fail2ban setup remains the same)
- name: Ensure fail2ban package is installed
  ansible.builtin.apt:
    name: fail2ban
    state: present
  when: security_fail2ban_enabled | default(false)
  tags: [security, fail2ban]

- name: Deploy fail2ban jail.local configuration from template
  ansible.builtin.template:
    src: jail.local.j2
    dest: /etc/fail2ban/jail.local
    owner: root
    group: root
    mode: '0644'
  when: security_fail2ban_enabled | default(false)
  notify: Restart fail2ban
  tags: [security, fail2ban]

- name: Ensure fail2ban service is enabled and running
  ansible.builtin.service:
    name: fail2ban
    state: started
    enabled: yes
  when: security_fail2ban_enabled | default(false)
  tags: [security, fail2ban]

--- End of: main.yml ---

----- Content of: main.yml -----
# /opt/architect_configs/roles/docker/tasks/main.yml
---
# Tasks for installing and configuring Docker CE

- name: Ensure prerequisites for Docker repo are installed
  ansible.builtin.apt:
    name:
      - ca-certificates
      - curl
    state: present
    update_cache: yes
  tags: [docker, packages]

# Remove conflicting docker.io package if present (best effort)
- name: Ensure conflicting docker.io package is removed
  ansible.builtin.apt:
    name:
      - docker.io
      - docker-doc
      - docker-compose # Older compose
      - podman-docker # Conflicting package
      - containerd
      - runc
    state: absent
  ignore_errors: yes # Ignore if packages are not installed
  tags: [docker, packages]

- name: Ensure Docker apt key directory exists
  ansible.builtin.file:
    path: /etc/apt/keyrings
    state: directory
    mode: '0755'
  tags: [docker, repo]

- name: Add Docker's official GPG key
  ansible.builtin.get_url:
    url: https://download.docker.com/linux/ubuntu/gpg
    dest: /etc/apt/keyrings/docker.asc
    mode: '0644'
    force: true # Overwrite if exists, ensures latest key
  tags: [docker, repo]

- name: Add Docker's official APT repository
  ansible.builtin.apt_repository:
    repo: "deb [arch=amd64 signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
    state: present
    filename: docker # Creates /etc/apt/sources.list.d/docker.list
    update_cache: yes
  tags: [docker, repo]

- name: Install Docker CE packages
  ansible.builtin.apt:
    name:
      - docker-ce
      - docker-ce-cli
      - containerd.io # Required by docker-ce
      - docker-buildx-plugin
      - docker-compose-plugin # Installs 'docker compose' command
    state: present
  tags: [docker, packages]
  notify: Restart docker # Handler needed

- name: Ensure docker group exists
  ansible.builtin.group:
    name: docker
    state: present
  tags: [docker, users]

- name: Add specified users to the docker group
  ansible.builtin.user:
    name: "{{ item }}"
    groups: docker
    append: yes # Add to existing groups
  loop: "{{ docker_users_to_group | default([]) }}" # Use variable from group_vars/all.yml
  tags: [docker, users]

- name: Ensure Docker service is enabled and running
  ansible.builtin.service:
    name: docker
    state: started
    enabled: yes
  tags: [docker, service]

--- End of: main.yml ---

----- Content of: main.yml -----
---
# File: roles/prometheus/tasks/main.yml
# Version: FINAL - Merged Local Fixes + Remote Volume Permission Fix + Hardcoded Src

- name: Ensure Prometheus directories exist
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    owner: root # Let root create dirs initially
    group: root
    mode: "0755"
  loop:
    - "{{ prometheus_compose_dir }}"
    - "{{ prometheus_config_dir }}"
    - "{{ prometheus_data_dir }}"
  become: yes
  tags: [prometheus, config]

- name: Deploy Prometheus configuration file (prometheus.yml)
  ansible.builtin.template:
    src: prometheus.yml.j2 # HARDCODED src path
    dest: "{{ prometheus_config_file_path }}"
    owner: root # Config needs to be readable by container user
    group: root # Group read often sufficient
    mode: "0644"
  become: true
  notify: Restart Prometheus container # Notify handler on config change
  tags: [prometheus, config]

- name: Ensure Docker Compose V2 is available
  ansible.builtin.command: docker compose version
  register: compose_version_check
  changed_when: false
  failed_when: compose_version_check.rc != 0
  become: true
  tags: [prometheus, docker, config]

- name: Deploy Prometheus docker-compose.yml file from template
  ansible.builtin.template:
    src: "{{ prometheus_template_name }}" # Var OK here, only config was failing
    dest: "{{ prometheus_compose_file_path }}"
    owner: root
    group: root
    mode: "0644"
  become: true
  notify: Restart Prometheus container # Notify handler on compose file change
  tags: [prometheus, config]

# --- BEGIN VOLUME PERMISSION FIX (from Remote) ---
- name: Ensure Prometheus data directory has correct permissions for container user
  ansible.builtin.file:
    path: "{{ prometheus_data_dir }}"
    state: directory
    owner: "65534" # UID for nobody (adjust if Prometheus container uses different UID)
    group: "65534" # GID for nogroup (adjust if Prometheus container uses different GID)
    mode: "0775" # Writable by owner/group
  become: true
  tags: [prometheus, config, permissions]
# --- END VOLUME PERMISSION FIX ---

- name: Ensure potentially conflicting container is absent (Local Fix Re-applied)
  community.docker.docker_container:
    name: prometheus
    state: absent
    force_kill: yes
  become: yes
  ignore_errors: yes
  tags: [prometheus, docker, run]

- name: Ensure Prometheus container is running via Docker Compose (Local Fix Re-applied)
  community.docker.docker_compose_v2:
    project_name: prometheus_stack
    definition: "{{ lookup('template', 'docker-compose.yml.j2') | from_yaml }}"
    state: present
    pull: missing
  become: yes
  register: compose_result
  tags: [prometheus, docker, run]
--- End of: main.yml ---

----- Content of: prometheus.yml.j2 -----
# File: roles/prometheus/templates/prometheus.yml.j2
# Version: FINAL - Merged Remote Changes + Correct Self-Scrape Port + Refined Jinja

global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: "prometheus"
    # Scrape Prometheus itself (inside Docker, uses internal port 9090)
    static_configs:
      - targets: ["localhost:9090"] # CORRECTED PORT to internal 9090

  - job_name: "node_exporter"
    # Scrape node_exporter based on inventory groups
    static_configs:
      - targets:
{%- set ne_targets = [] %} {# Initialize list #}
{%- set ne_port = node_exporter_port | default(9100) %} {# Get default port from vars #}
{%- for host in (groups['server'] | default([])) + (groups['clients'] | default([])) %} {# Combine server & client groups safely #}
{%-   set host_ip = hostvars[host]['ansible_host'] | default(host) %} {# Use connection IP or inventory name #}
{%-   set _ = ne_targets.append(host_ip ~ ':' ~ ne_port) %} {# Append 'host:port' #}
{%- endfor %}
{{ ne_targets | to_yaml | indent(8) }} {# Output list as YAML #}

# Add alerting rules configuration if needed
# rule_files:
#   - "/etc/prometheus/rules/*.rules.yml"
--- End of: prometheus.yml.j2 ---

----- Content of: docker-compose.yml.j2 -----
# File: roles/prometheus/templates/docker-compose.yml.j2
# Version: FINAL - Merged Remote Changes + Correct Network Config

# Define network locally scoped to this project (prometheus_stack)
# Loki/Grafana will reference this as prometheus_stack_monitoring_net externally.
networks:
  monitoring_net: # This name gets prefixed by project_name ('prometheus_stack')
    driver: bridge

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus # Explicit name
    user: "65534:65534" # Run as nobody:nogroup
    volumes:
      # Mount config file rendered by Ansible (read-only)
      - "{{ prometheus_config_file_path }}:/etc/prometheus/prometheus.yml:ro"
      # Mount data volume from HOST - permissions handled by task in main.yml
      - "{{ prometheus_data_dir }}:/prometheus"
    ports:
      # Use variable from play vars (site.yml) for host port
      - "{{ prometheus_port }}:9090" # Uses 'prometheus_port' consistent with site.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks: # Connect service to the network defined above
      - monitoring_net
    labels:
      org.label-schema.group: "monitoring"
      managed-by: "ansible-chimera-prime"
--- End of: docker-compose.yml.j2 ---

=========================================

### Network Information (Control Node) ###
----- IP Addresses -----
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: enp5s0f1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether c4:54:44:3a:3c:6c brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.96/24 brd 192.168.0.255 scope global dynamic noprefixroute enp5s0f1
       valid_lft 168882sec preferred_lft 168882sec
    inet6 fd67:f474:67ad:db1:3706:1ca0:d198:dc39/64 scope global temporary dynamic 
       valid_lft 600882sec preferred_lft 82210sec
    inet6 fd67:f474:67ad:db1:ad39:ca36:3229:a132/64 scope global dynamic mngtmpaddr noprefixroute 
       valid_lft 2591993sec preferred_lft 604793sec
    inet6 2600:8803:a001:d800::3dcb/128 scope global dynamic noprefixroute 
       valid_lft 82481sec preferred_lft 82481sec
    inet6 2600:8803:a001:d800:ac17:f631:6f83:2176/64 scope global temporary dynamic 
       valid_lft 299sec preferred_lft 299sec
    inet6 2600:8803:a001:d800:b42d:dc06:7b3a:4cc6/64 scope global dynamic mngtmpaddr noprefixroute 
       valid_lft 299sec preferred_lft 299sec
    inet6 fe80::cdb5:6800:a954:eed1/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: br-58584849cc8d: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 5a:cc:7b:3b:8e:59 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-58584849cc8d
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 8a:2b:40:b7:95:69 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
----- Listening Ports (TCP/UDP) -----
Netid State  Recv-Q Send-Q                        Local Address:Port  Peer Address:PortProcess                                 
udp   UNCONN 0      0                             127.0.0.53%lo:53         0.0.0.0:*                                           
udp   UNCONN 0      0                                 127.0.0.1:323        0.0.0.0:*                                           
udp   UNCONN 0      0                                   0.0.0.0:35920      0.0.0.0:*                                           
udp   UNCONN 0      0                                   0.0.0.0:5353       0.0.0.0:*                                           
udp   UNCONN 0      0                                         *:39697            *:*    users:(("firefox-bin",pid=3651,fd=173))
udp   UNCONN 0      0                                      [::]:48082         [::]:*                                           
udp   UNCONN 0      0                                     [::1]:323           [::]:*                                           
udp   UNCONN 0      0      [fe80::cdb5:6800:a954:eed1]%enp5s0f1:546           [::]:*                                           
udp   UNCONN 0      0                                         *:58317            *:*    users:(("firefox-bin",pid=3651,fd=201))
udp   UNCONN 0      0                                         *:35615            *:*    users:(("firefox-bin",pid=3651,fd=205))
udp   UNCONN 0      0                                         *:52494            *:*    users:(("firefox-bin",pid=3651,fd=169))
udp   UNCONN 0      0                                         *:37203            *:*    users:(("firefox-bin",pid=3651,fd=127))
udp   UNCONN 0      0                                      [::]:5353          [::]:*                                           
udp   UNCONN 0      0                                         *:38440            *:*    users:(("firefox-bin",pid=3651,fd=113))
udp   UNCONN 0      0                                         *:47178            *:*    users:(("firefox-bin",pid=3651,fd=153))
tcp   LISTEN 0      4096                          127.0.0.53%lo:53         0.0.0.0:*                                           
tcp   LISTEN 0      511                               127.0.0.1:46661      0.0.0.0:*    users:(("code",pid=2962592,fd=58))     
tcp   LISTEN 0      511                               127.0.0.1:44821      0.0.0.0:*    users:(("code",pid=4017805,fd=41))     
tcp   LISTEN 0      4096                              127.0.0.1:11434      0.0.0.0:*                                           
tcp   LISTEN 0      128                               127.0.0.1:631        0.0.0.0:*                                           
tcp   LISTEN 0      128                                 0.0.0.0:22         0.0.0.0:*                                           
tcp   LISTEN 0      511                               127.0.0.1:33227      0.0.0.0:*    users:(("code",pid=4017805,fd=40))     
tcp   LISTEN 0      511                               127.0.0.1:34661      0.0.0.0:*    users:(("code",pid=4017805,fd=55))     
tcp   LISTEN 0      511                               127.0.0.1:33921      0.0.0.0:*    users:(("code",pid=2963729,fd=23))     
tcp   LISTEN 0      128                               127.0.0.1:42399      0.0.0.0:*    users:(("ssh",pid=4081528,fd=5))       
tcp   LISTEN 0      50                       [::ffff:127.0.0.1]:64120            *:*    users:(("java",pid=2963624,fd=16))     
tcp   LISTEN 0      4096                                      *:9090             *:*                                           
tcp   LISTEN 0      4096                                      *:9100             *:*                                           
tcp   LISTEN 0      128                                    [::]:22            [::]:*                                           
tcp   LISTEN 0      128                                   [::1]:631           [::]:*                                           
tcp   LISTEN 0      128                                   [::1]:42399         [::]:*    users:(("ssh",pid=4081528,fd=4))       
=========================================

### Ansible Lint Check (Optional) ###
# 'ansible-lint' command not found. Skipping lint check.
=========================================

### Nexus Reality Scan Complete ###
Provide the *entire* output above when requested.
